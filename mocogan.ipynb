{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5466,"status":"ok","timestamp":1742741596150,"user":{"displayName":"Alexei Klimovich","userId":"06703062093340586073"},"user_tz":-180},"id":"-0oGzzL1jAS7","outputId":"0e4cab42-0726-45ed-8bae-b26600e040a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'mocogan'...\n","remote: Enumerating objects: 8366, done.\u001b[K\n","remote: Total 8366 (delta 0), reused 0 (delta 0), pack-reused 8366 (from 1)\u001b[K\n","Receiving objects: 100% (8366/8366), 83.29 MiB | 27.56 MiB/s, done.\n","Resolving deltas: 100% (83/83), done.\n","0  1\n"]}],"source":["#We dogitclonebutwejust usethe data. We don't use the code.\n","!git clone https://github.com/sergeytulyakov/mocogan.git\n","!ls mocogan/data/shapes\n"]},{"cell_type":"markdown","metadata":{"id":"YgwXFLoEmwLK"},"source":["#Import"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13134,"status":"ok","timestamp":1742741611932,"user":{"displayName":"Alexei Klimovich","userId":"06703062093340586073"},"user_tz":-180},"id":"CHpj7YYaLALL"},"outputs":[],"source":["import os\n","\n","import PIL\n","\n","import functools\n","import IPython.display\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import transforms"]},{"cell_type":"markdown","metadata":{"id":"vQdi8PTDTWRX"},"source":["# models\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ip6eZ_RiTZmJ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.utils.data\n","from torch.autograd import Variable\n","\n","import numpy as np\n","\n","if torch.cuda.is_available():\n","    T = torch.cuda\n","else:\n","    T = torch\n","\n","\n","class Noise(nn.Module):\n","    def __init__(self, use_noise, sigma=0.2):\n","        super(Noise, self).__init__()\n","        self.use_noise = use_noise\n","        self.sigma = sigma\n","\n","    def forward(self, x):\n","        if self.use_noise:\n","            return x + self.sigma * Variable(T.FloatTensor(x.size()).normal_(), requires_grad=False)\n","        return x\n","\n","\n","class ImageDiscriminator(nn.Module):\n","    def __init__(self, n_channels, ndf=64, use_noise=False, noise_sigma=None):\n","        super(ImageDiscriminator, self).__init__()\n","\n","        self.use_noise = use_noise\n","\n","        self.main = nn.Sequential(\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv2d(n_channels, ndf, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n","        )\n","\n","    def forward(self, input):\n","        h = self.main(input).squeeze()\n","        return h, None\n","\n","\n","class PatchImageDiscriminator(nn.Module):\n","    def __init__(self, n_channels, ndf=64, use_noise=False, noise_sigma=None):\n","        super(PatchImageDiscriminator, self).__init__()\n","\n","        self.use_noise = use_noise\n","\n","        self.main = nn.Sequential(\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv2d(n_channels, ndf, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv2d(ndf * 4, 1, 4, 2, 1, bias=False),\n","        )\n","\n","    def forward(self, input):\n","        h = self.main(input).squeeze()\n","        return h, None\n","\n","\n","class PatchVideoDiscriminator(nn.Module):\n","    def __init__(self, n_channels, n_output_neurons=1, bn_use_gamma=True, use_noise=False, noise_sigma=None, ndf=64):\n","        super(PatchVideoDiscriminator, self).__init__()\n","\n","        self.n_channels = n_channels\n","        self.n_output_neurons = n_output_neurons\n","        self.use_noise = use_noise\n","        self.bn_use_gamma = bn_use_gamma\n","\n","        self.main = nn.Sequential(\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv3d(n_channels, ndf, 4, stride=(1, 2, 2), padding=(0, 1, 1), bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv3d(ndf, ndf * 2, 4, stride=(1, 2, 2), padding=(0, 1, 1), bias=False),\n","            nn.BatchNorm3d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv3d(ndf * 2, ndf * 4, 4, stride=(1, 2, 2), padding=(0, 1, 1), bias=False),\n","            nn.BatchNorm3d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv3d(ndf * 4, 1, 4, stride=(1, 2, 2), padding=(0, 1, 1), bias=False),\n","        )\n","\n","    def forward(self, input):\n","        h = self.main(input).squeeze()\n","        return h, None\n","\n","\n","class VideoDiscriminator(nn.Module):\n","    def __init__(self, n_channels, n_output_neurons=1, bn_use_gamma=True, use_noise=False, noise_sigma=None, ndf=64):\n","        super(VideoDiscriminator, self).__init__()\n","\n","        self.n_channels = n_channels\n","        self.n_output_neurons = n_output_neurons\n","        self.use_noise = use_noise\n","        self.bn_use_gamma = bn_use_gamma\n","\n","        self.main = nn.Sequential(\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv3d(n_channels, ndf, 4, stride=(1, 2, 2), padding=(0, 1, 1), bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv3d(ndf, ndf * 2, 4, stride=(1, 2, 2), padding=(0, 1, 1), bias=False),\n","            nn.BatchNorm3d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv3d(ndf * 2, ndf * 4, 4, stride=(1, 2, 2), padding=(0, 1, 1), bias=False),\n","            nn.BatchNorm3d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            Noise(use_noise, sigma=noise_sigma),\n","            nn.Conv3d(ndf * 4, ndf * 8, 4, stride=(1, 2, 2), padding=(0, 1, 1), bias=False),\n","            nn.BatchNorm3d(ndf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv3d(ndf * 8, n_output_neurons, 4, 1, 0, bias=False),\n","        )\n","\n","    def forward(self, input):\n","        h = self.main(input).squeeze()\n","        return h, None\n","\n","\n","class CategoricalVideoDiscriminator(VideoDiscriminator):\n","    def __init__(self, n_channels, dim_categorical, n_output_neurons=1, use_noise=False, noise_sigma=None):\n","        super(CategoricalVideoDiscriminator, self).__init__(\n","            n_channels=n_channels,\n","            n_output_neurons=n_output_neurons + dim_categorical,\n","            use_noise=use_noise,\n","            noise_sigma=noise_sigma\n","        )\n","        self.dim_categorical = dim_categorical\n","\n","    def split(self, input):\n","        return input[:, :input.size(1) - self.dim_categorical], input[:, input.size(1) - self.dim_categorical:]\n","\n","    def forward(self, input):\n","        h, _ = super(CategoricalVideoDiscriminator, self).forward(input)\n","        labels, categ = self.split(h)\n","        return labels, categ\n","\n","\n","class VideoGenerator(nn.Module):\n","    def __init__(\n","        self, n_channels, dim_z_content, dim_z_category, dim_z_motion, video_length, ngf=64\n","    ):\n","        super(VideoGenerator, self).__init__()\n","\n","        self.n_channels = n_channels\n","        self.dim_z_content = dim_z_content\n","        self.dim_z_category = dim_z_category\n","        self.dim_z_motion = dim_z_motion\n","        self.video_length = video_length\n","\n","        dim_z = dim_z_motion + dim_z_category + dim_z_content\n","        self.recurrent = nn.GRUCell(dim_z_motion, dim_z_motion)\n","\n","        self.main = nn.Sequential(\n","            nn.ConvTranspose2d(dim_z, ngf * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(ngf, self.n_channels, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def sample_z_m(self, num_samples, video_len=None):\n","        video_len = video_len if video_len is not None else self.video_length\n","        h_t = [self.get_gru_initial_state(num_samples)]\n","\n","        for frame_num in range(video_len):\n","            e_t = self.get_iteration_noise(num_samples)\n","            h_t.append(self.recurrent(e_t, h_t[-1]))\n","\n","        z_m_t = [h_k.view(-1, 1, self.dim_z_motion) for h_k in h_t]\n","        z_m = torch.cat(z_m_t[1:], dim=1).view(-1, self.dim_z_motion)\n","\n","        return z_m\n","\n","    def sample_z_categ(self, num_samples, video_len):\n","        video_len = video_len if video_len is not None else self.video_length\n","\n","        if self.dim_z_category <= 0:\n","            return None, np.zeros(num_samples)\n","\n","        classes_to_generate = np.random.randint(self.dim_z_category, size=num_samples)\n","        one_hot = np.zeros((num_samples, self.dim_z_category), dtype=np.float32)\n","        one_hot[np.arange(num_samples), classes_to_generate] = 1\n","        one_hot_video = np.repeat(one_hot, video_len, axis=0)\n","\n","        one_hot_video = torch.from_numpy(one_hot_video)\n","\n","        if torch.cuda.is_available():\n","            one_hot_video = one_hot_video.cuda()\n","\n","        return Variable(one_hot_video), classes_to_generate\n","\n","    def sample_z_content(self, num_samples, video_len=None):\n","        video_len = video_len if video_len is not None else self.video_length\n","\n","        content = np.random.normal(0, 1, (num_samples, self.dim_z_content)).astype(np.float32)\n","        content = np.repeat(content, video_len, axis=0)\n","        content = torch.from_numpy(content)\n","        if torch.cuda.is_available():\n","            content = content.cuda()\n","        return Variable(content)\n","\n","    def sample_z_video(self, num_samples, video_len=None):\n","        z_content = self.sample_z_content(num_samples, video_len)\n","        z_category, z_category_labels = self.sample_z_categ(num_samples, video_len)\n","        z_motion = self.sample_z_m(num_samples, video_len)\n","\n","        if z_category is not None:\n","            z = torch.cat([z_content, z_category, z_motion], dim=1)\n","        else:\n","            z = torch.cat([z_content, z_motion], dim=1)\n","\n","        return z, z_category_labels\n","\n","    def sample_videos(self, num_samples, video_len=None):\n","        video_len = video_len if video_len is not None else self.video_length\n","\n","        z, z_category_labels = self.sample_z_video(num_samples, video_len)\n","\n","        h = self.main(z.view(z.size(0), z.size(1), 1, 1))\n","        h = h.view(h.size(0) // video_len, video_len, self.n_channels, h.size(3), h.size(3))\n","\n","        z_category_labels = torch.from_numpy(z_category_labels)\n","\n","        if torch.cuda.is_available():\n","            z_category_labels = z_category_labels.cuda()\n","\n","        h = h.permute(0, 2, 1, 3, 4)\n","        return h, Variable(z_category_labels, requires_grad=False)\n","\n","    def sample_images(self, num_samples):\n","        z, z_category_labels = self.sample_z_video(num_samples * self.video_length * 2)\n","\n","        j = np.sort(np.random.choice(z.size(0), num_samples, replace=False)).astype(np.int64)\n","        z = z[j, ::]\n","        z = z.view(z.size(0), z.size(1), 1, 1)\n","        h = self.main(z)\n","\n","        return h, None\n","\n","    def get_gru_initial_state(self, num_samples):\n","        return Variable(T.FloatTensor(num_samples, self.dim_z_motion).normal_())\n","\n","    def get_iteration_noise(self, num_samples):\n","        return Variable(T.FloatTensor(num_samples, self.dim_z_motion).normal_())"]},{"cell_type":"markdown","metadata":{"id":"7A1RtrF0TsIg"},"source":["#data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1yrY5U_LO6X"},"outputs":[],"source":["import os\n","import tqdm\n","import pickle\n","import numpy as np\n","import torch.utils.data\n","from torchvision.datasets import ImageFolder\n","import PIL\n","\n","\n","class VideoFolderDataset(torch.utils.data.Dataset):\n","    def __init__(self, folder, cache, min_len=32):\n","        dataset = ImageFolder(folder)\n","        self.total_frames = 0\n","        self.lengths = []\n","        self.images = []\n","\n","        if cache is not None and os.path.exists(cache) and os.path.getsize(cache) != 0:\n","            with open(cache, 'rb') as f:\n","                self.images, self.lengths = pickle.load(f)\n","        else:\n","            for idx, (im, categ) in enumerate(\n","                    tqdm.tqdm(dataset, desc=\"Counting total number of frames\")):\n","                img_path, _ = dataset.imgs[idx]\n","                shorter, longer = min(im.width, im.height), max(im.width, im.height)\n","                length = longer // shorter\n","                if length >= min_len:\n","                    self.images.append((img_path, categ))\n","                    self.lengths.append(length)\n","\n","            if cache is not None:\n","                with open(cache, 'wb') as f:\n","                    pickle.dump((self.images, self.lengths), f)\n","\n","        self.cumsum = np.cumsum([0] + self.lengths)\n","        print(\"Total number of frames {}\".format(np.sum(self.lengths)))\n","\n","    def __getitem__(self, item):\n","        path, label = self.images[item]\n","        im = PIL.Image.open(path)\n","        return im, label\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","\n","class ImageDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, transform=None):\n","        self.dataset = dataset\n","        self.transforms = transform if transform is not None else lambda x: x\n","\n","    def __getitem__(self, item):\n","        if item != 0:\n","            video_id = np.searchsorted(self.dataset.cumsum, item) - 1\n","            frame_num = item - self.dataset.cumsum[video_id] - 1\n","        else:\n","            video_id = 0\n","            frame_num = 0\n","\n","        video, target = self.dataset[video_id]\n","        video = np.array(video)\n","\n","        horizontal = video.shape[1] > video.shape[0]\n","\n","        if horizontal:\n","            i_from, i_to = video.shape[0] * frame_num, video.shape[0] * (frame_num + 1)\n","            frame = video[:, i_from: i_to, ::]\n","        else:\n","            i_from, i_to = video.shape[1] * frame_num, video.shape[1] * (frame_num + 1)\n","            frame = video[i_from: i_to, :, ::]\n","\n","        if frame.shape[0] == 0:\n","            print(\"video {}. From {} to {}. num {}\".format(video.shape, i_from, i_to, item))\n","\n","        return {\"images\": self.transforms(frame), \"categories\": target}\n","\n","    def __len__(self):\n","        return self.dataset.cumsum[-1]\n","\n","\n","class VideoDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, video_length, every_nth=1, transform=None):\n","        self.dataset = dataset\n","        self.video_length = video_length\n","        self.every_nth = every_nth\n","        self.transforms = transform if transform is not None else lambda x: x\n","\n","    def __getitem__(self, item):\n","        video, target = self.dataset[item]\n","        video = np.array(video)\n","\n","        horizontal = video.shape[1] > video.shape[0]\n","        shorter, longer = min(video.shape[0], video.shape[1]), max(video.shape[0], video.shape[1])\n","        video_len = longer // shorter\n","\n","        # videos can be of various length, we randomly sample sub-sequences\n","        if video_len >= self.video_length * self.every_nth:\n","            needed = self.every_nth * (self.video_length - 1)\n","            gap = video_len - needed\n","            start = 0 if gap == 0 else np.random.randint(0, gap, 1)[0]\n","            subsequence_idx = np.linspace(start, start + needed, self.video_length, endpoint=True, dtype=np.int32)\n","        elif video_len >= self.video_length:\n","            subsequence_idx = np.arange(0, self.video_length)\n","        else:\n","            raise Exception(\"Length is too short id - {}, len - {}\").format(self.dataset[item], video_len)\n","\n","        frames = np.split(video, video_len, axis=1 if horizontal else 0)\n","        selected = np.array([frames[s_id] for s_id in subsequence_idx])\n","\n","        return {\"images\": self.transforms(selected), \"categories\": target}\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","\n","class ImageSampler(torch.utils.data.Dataset):\n","    def __init__(self, dataset, transform=None):\n","        self.dataset = dataset\n","        self.transforms = transform\n","\n","    def __getitem__(self, index):\n","        result = {}\n","        for k in self.dataset.keys:\n","            result[k] = np.take(self.dataset.get_data()[k], index, axis=0)\n","\n","        if self.transforms is not None:\n","            for k, transform in self.transforms.items():\n","                result[k] = transform(result[k])\n","\n","        return result\n","\n","    def __len__(self):\n","        return self.dataset.get_data()[self.dataset.keys[0]].shape[0]\n","\n","\n","class VideoSampler(torch.utils.data.Dataset):\n","    def __init__(self, dataset, video_length, every_nth=1, transform=None):\n","        self.dataset = dataset\n","        self.video_length = video_length\n","        self.unique_ids = np.unique(self.dataset.get_data()['video_ids'])\n","        self.every_nth = every_nth\n","        self.transforms = transform\n","\n","    def __getitem__(self, item):\n","        result = {}\n","        ids = self.dataset.get_data()['video_ids'] == self.unique_ids[item]\n","        ids = np.squeeze(np.squeeze(np.argwhere(ids)))\n","        for k in self.dataset.keys:\n","            result[k] = np.take(self.dataset.get_data()[k], ids, axis=0)\n","\n","        subsequence_idx = None\n","        print(result[k].shape[0])\n","\n","        # videos can be of various length, we randomly sample sub-sequences\n","        if result[k].shape[0] > self.video_length:\n","            needed = self.every_nth * (self.video_length - 1)\n","            gap = result[k].shape[0] - needed\n","            start = 0 if gap == 0 else np.random.randint(0, gap, 1)[0]\n","            subsequence_idx = np.linspace(start, start + needed, self.video_length, endpoint=True, dtype=np.int32)\n","        elif result[k].shape[0] == self.video_length:\n","            subsequence_idx = np.arange(0, self.video_length)\n","        else:\n","            print(\"Length is too short id - {}, len - {}\".format(self.unique_ids[item], result[k].shape[0]))\n","\n","        if subsequence_idx:\n","            for k in self.dataset.keys:\n","                result[k] = np.take(result[k], subsequence_idx, axis=0)\n","        else:\n","            print(result[self.dataset.keys[0]].shape)\n","\n","        if self.transforms is not None:\n","            for k, transform in self.transforms.items():\n","                result[k] = transform(result[k])\n","\n","        return result\n","\n","    def __len__(self):\n","        return len(self.unique_ids)"]},{"cell_type":"markdown","metadata":{"id":"i6rpPgozYL9M"},"source":["#Logger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"goW4VoBSYOoX"},"outputs":[],"source":["import PIL\n","import tensorflow as tf\n","import numpy as np\n","\n","try:\n","    from StringIO import StringIO  # Python 2.7\n","except ImportError:\n","    from io import BytesIO  # Python 3.x\n","\n","\n","class Logger(object):\n","    def __init__(self, log_dir, suffix=None):\n","        self.writer = tf.summary.create_file_writer(log_dir, filename_suffix=suffix)\n","\n","    def scalar_summary(self, tag, value, step):\n","        with self.writer.as_default():\n","            tf.summary.scalar(tag, value, step=step)\n","            self.writer.flush()  # Optional: flush the writer to ensure the summary is written\n","\n","    def image_summary(self, tag, images, step):\n","        img_summaries = []\n","        for i, img in enumerate(images):\n","            # Convert the image to a format suitable for saving\n","            img = np.clip(img, 0, 255).astype(np.uint8)  # Ensure values are in [0, 255]\n","\n","            # Write the image to a string\n","            try:\n","                s = StringIO()\n","            except:\n","                s = BytesIO()\n","\n","            # Create a PIL image from the numpy array\n","            pil_img = PIL.Image.fromarray(img)\n","            pil_img.save(s, format=\"png\")  # Save the image to the BytesIO object\n","\n","            # Convert the byte string to a tensor\n","            image_tensor = tf.image.decode_png(s.getvalue())\n","\n","            # Append the image summary to the img_summaries list\n","            img_summaries.append((f'{tag}/{i}', image_tensor))\n","\n","        # Write the summaries to TensorBoard\n","        with self.writer.as_default():\n","            for img_tag, img_tensor in img_summaries:\n","                tf.summary.image(img_tag, tf.expand_dims(img_tensor, 0), step=step)\n","\n","        self.writer.flush()\n","\n","    def video_summary(self, tag, videos, step):\n","        sh = list(videos.shape)\n","        sh[-1] = 1\n","\n","        separator = np.zeros(sh, dtype=videos.dtype)\n","        videos = np.concatenate([videos, separator], axis=-1)\n","\n","        for i, vid in enumerate(videos):\n","            # Concat a video\n","            try:\n","                s = StringIO()\n","            except:\n","                s = BytesIO()\n","\n","            v = vid.transpose(1, 2, 3, 0)\n","            v = [np.squeeze(f) for f in np.split(v, v.shape[0], axis=0)]\n","            img = np.concatenate(v, axis=1)[:, :-1, :]\n","\n","            # Нормализуем значения\n","            img = (img * 255).astype(np.uint8)\n","\n","            # Convert and save image using Pillow\n","            pil_img = PIL.Image.fromarray(np.clip(img, 0, 255).astype(np.uint8))  # Ensure correct format\n","            pil_img.save(s, format=\"png\")  # Save to BytesIO\n","            encoded_image_string = s.getvalue()  # Get the byte value of the image\n","\n","            # Write the summary directly to TensorBoard\n","            with self.writer.as_default():\n","                tf.summary.image(f'{tag}/{i}', [tf.image.decode_png(encoded_image_string)], step=step)\n","\n","        self.writer.flush()\n"]},{"cell_type":"markdown","metadata":{"id":"Id_s2_xUTk8l"},"source":["#Trainer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbzPeeacTnda"},"outputs":[],"source":["import os\n","import time\n","\n","from tqdm import tqdm\n","\n","import numpy as np\n","\n","import torch\n","from torch import nn\n","\n","from torch.autograd import Variable\n","import torch.optim as optim\n","\n","if torch.cuda.is_available():\n","    T = torch.cuda\n","else:\n","    T = torch\n","\n","\n","def images_to_numpy(tensor):\n","    generated = tensor.data.cpu().numpy().transpose(0, 2, 3, 1)\n","    generated[generated < -1] = -1\n","    generated[generated > 1] = 1\n","    generated = (generated + 1) / 2 * 255\n","    return generated.astype('uint8')\n","\n","\n","def videos_to_numpy(tensor):\n","    generated = tensor.data.cpu().numpy().transpose(0, 1, 2, 3, 4)\n","    generated[generated < -1] = -1\n","    generated[generated > 1] = 1\n","    generated = (generated + 1) / 2 * 255\n","    return generated.astype('uint8')\n","\n","\n","def one_hot_to_class(tensor):\n","    a, b = np.nonzero(tensor)\n","    return np.unique(b).astype(np.int32)\n","\n","\n","class Trainer(object):\n","    def __init__(\n","        self,\n","        image_sampler,\n","        video_sampler,\n","        log_interval,\n","        train_batches,\n","        log_folder,\n","        use_cuda=False,\n","        use_infogan=True,\n","        use_categories=True\n","    ):\n","        self.use_categories = use_categories\n","\n","        self.gan_criterion = nn.BCEWithLogitsLoss()\n","        self.category_criterion = nn.CrossEntropyLoss()\n","\n","        self.image_sampler = image_sampler\n","        self.video_sampler = video_sampler\n","\n","        self.video_batch_size = self.video_sampler.batch_size\n","        self.image_batch_size = self.image_sampler.batch_size\n","\n","        self.log_interval = log_interval\n","        self.train_batches = train_batches\n","\n","        self.log_folder = log_folder\n","\n","        self.use_cuda = use_cuda\n","        self.use_infogan = use_infogan\n","\n","        self.image_enumerator = None\n","        self.video_enumerator = None\n","\n","    @staticmethod\n","    def ones_like(tensor, val=1.):\n","        return Variable(T.FloatTensor(tensor.size()).fill_(val), requires_grad=False)\n","\n","    @staticmethod\n","    def zeros_like(tensor, val=0.):\n","        return Variable(T.FloatTensor(tensor.size()).fill_(val), requires_grad=False)\n","\n","    def compute_gan_loss(self, discriminator, sample_true, sample_fake, is_video):\n","        real_batch = sample_true()\n","\n","        batch_size = real_batch['images'].size(0)\n","        fake_batch, generated_categories = sample_fake(batch_size)\n","\n","        real_labels, real_categorical = discriminator(Variable(real_batch['images']))\n","        fake_labels, fake_categorical = discriminator(fake_batch)\n","\n","        fake_gt, real_gt = self.get_gt_for_discriminator(batch_size, real=0.)\n","\n","        l_discriminator = self.gan_criterion(real_labels, real_gt) + \\\n","                          self.gan_criterion(fake_labels, fake_gt)\n","\n","        # update image discriminator here\n","\n","        # sample again for videos\n","\n","        # update video discriminator\n","\n","        # sample again\n","        # - videos\n","        # - images\n","\n","        # l_vidoes + l_images -> l\n","        # l.backward()\n","        # opt.step()\n","\n","\n","        #  sample again and compute for generator\n","\n","        fake_gt = self.get_gt_for_generator(batch_size)\n","        # to real_gt\n","        l_generator = self.gan_criterion(fake_labels, fake_gt)\n","\n","        if is_video:\n","\n","            # Ask the video discriminator to learn categories from training videos\n","            categories_gt = Variable(torch.squeeze(real_batch['categories'].long()))\n","            l_discriminator += self.category_criterion(real_categorical, categories_gt)\n","\n","            if self.use_infogan:\n","                # Ask the generator to generate categories recognizable by the discriminator\n","                l_generator += self.category_criterion(fake_categorical, generated_categories)\n","\n","        return l_generator, l_discriminator\n","\n","    def sample_real_image_batch(self):\n","        if self.image_enumerator is None:\n","            self.image_enumerator = enumerate(self.image_sampler)\n","\n","        batch_idx, batch = next(self.image_enumerator)\n","        b = batch\n","        if self.use_cuda:\n","            for k, v in batch.items():\n","                b[k] = v.cuda()\n","\n","        if batch_idx == len(self.image_sampler) - 1:\n","            self.image_enumerator = enumerate(self.image_sampler)\n","\n","        return b\n","\n","    def sample_real_video_batch(self):\n","        if self.video_enumerator is None:\n","            self.video_enumerator = enumerate(self.video_sampler)\n","\n","        batch_idx, batch = next(self.video_enumerator)\n","        b = batch\n","        if self.use_cuda:\n","            for k, v in batch.items():\n","                b[k] = v.cuda()\n","\n","        if batch_idx == len(self.video_sampler) - 1:\n","            self.video_enumerator = enumerate(self.video_sampler)\n","\n","        return b\n","\n","    def train_discriminator(self, discriminator, sample_true, sample_fake, opt, batch_size, use_categories):\n","        opt.zero_grad()\n","\n","        real_batch = sample_true()\n","        batch = Variable(real_batch['images'], requires_grad=False)\n","\n","        # util.show_batch(batch.data)\n","\n","        fake_batch, generated_categories = sample_fake(batch_size)\n","\n","        real_labels, real_categorical = discriminator(batch)\n","        fake_labels, fake_categorical = discriminator(fake_batch.detach())\n","\n","        ones = self.ones_like(real_labels)\n","        zeros = self.zeros_like(fake_labels)\n","\n","        l_discriminator = self.gan_criterion(real_labels, ones) + \\\n","                          self.gan_criterion(fake_labels, zeros)\n","\n","        if use_categories:\n","            # Ask the video discriminator to learn categories from training videos\n","            categories_gt = Variable(torch.squeeze(real_batch['categories'].long()), requires_grad=False)\n","            l_discriminator += self.category_criterion(real_categorical.squeeze(), categories_gt)\n","\n","        l_discriminator.backward()\n","        opt.step()\n","\n","        return l_discriminator\n","\n","    def train_generator(\n","        self,\n","        image_discriminator,\n","        video_discriminator,\n","        sample_fake_images,\n","        sample_fake_videos,\n","        opt\n","    ):\n","        opt.zero_grad()\n","\n","        # train on images\n","\n","        fake_batch, generated_categories = sample_fake_images(self.image_batch_size)\n","        fake_labels, fake_categorical = image_discriminator(fake_batch)\n","        all_ones = self.ones_like(fake_labels)\n","\n","        l_generator = self.gan_criterion(fake_labels, all_ones)\n","\n","        # train on videos\n","\n","        fake_batch, generated_categories = sample_fake_videos(self.video_batch_size)\n","        fake_labels, fake_categorical = video_discriminator(fake_batch)\n","        all_ones = self.ones_like(fake_labels)\n","\n","        l_generator += self.gan_criterion(fake_labels, all_ones)\n","\n","        if self.use_infogan:\n","            # Ask the generator to generate categories recognizable by the discriminator\n","            l_generator += self.category_criterion(fake_categorical.squeeze(), generated_categories)\n","\n","        l_generator.backward()\n","        opt.step()\n","\n","        return l_generator\n","\n","    def train(self, generator, image_discriminator, video_discriminator, num_epochs):\n","        if self.use_cuda:\n","            generator.cuda()\n","            image_discriminator.cuda()\n","            video_discriminator.cuda()\n","\n","        logger = Logger(self.log_folder)\n","\n","        # create optimizers\n","        opt_generator = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=0.00001)\n","        opt_image_discriminator = optim.Adam(\n","            image_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=0.00001\n","        )\n","        opt_video_discriminator = optim.Adam(\n","            video_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=0.00001\n","        )\n","\n","        # training loop\n","\n","        def sample_fake_image_batch(batch_size):\n","            return generator.sample_images(batch_size)\n","\n","        def sample_fake_video_batch(batch_size):\n","            return generator.sample_videos(batch_size)\n","\n","        def init_logs():\n","            return {'l_gen': 0, 'l_image_dis': 0, 'l_video_dis': 0}\n","\n","        for epoch in range(num_epochs):\n","\n","            batch_num = 0\n","            logs = init_logs()\n","            epoch_logs = init_logs()  # Accumulate losses for the entire epoch\n","            start_time = time.time()\n","\n","            with tqdm(total=self.train_batches, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n","                while batch_num < self.train_batches:\n","                    generator.train()\n","                    image_discriminator.train()\n","                    video_discriminator.train()\n","\n","                    opt_generator.zero_grad()\n","                    opt_video_discriminator.zero_grad()\n","\n","                    # train image discriminator\n","                    l_image_dis = self.train_discriminator(\n","                        image_discriminator, self.sample_real_image_batch,\n","                        sample_fake_image_batch, opt_image_discriminator,\n","                        self.image_batch_size, use_categories=False\n","                    )\n","\n","                    # train video discriminator\n","                    l_video_dis = self.train_discriminator(\n","                        video_discriminator, self.sample_real_video_batch,\n","                        sample_fake_video_batch, opt_video_discriminator,\n","                        self.video_batch_size, use_categories=self.use_categories\n","                    )\n","\n","                    # train generator\n","                    l_gen = self.train_generator(\n","                        image_discriminator, video_discriminator,\n","                        sample_fake_image_batch, sample_fake_video_batch,\n","                        opt_generator\n","                    )\n","\n","                    logs['l_gen'] += l_gen.item()\n","                    logs['l_image_dis'] += l_image_dis.item()\n","                    logs['l_video_dis'] += l_video_dis.item()\n","\n","                    # Accumulate epoch losses\n","                    epoch_logs['l_gen'] += l_gen.item()\n","                    epoch_logs['l_image_dis'] += l_image_dis.item()\n","                    epoch_logs['l_video_dis'] += l_video_dis.item()\n","\n","                    batch_num += 1\n","\n","                    if batch_num % self.log_interval == 0:\n","                        log_string = \"Batch %d\" % batch_num\n","                        for k, v in logs.items():\n","                            log_string += \" [%s] %5.3f\" % (k, v / self.log_interval)\n","\n","                        log_string += \". Took %5.2f\" % (time.time() - start_time)\n","\n","                        print(log_string)\n","\n","                        for tag, value in logs.items():\n","                            logger.scalar_summary(tag, value / self.log_interval, batch_num)\n","\n","                        logs = init_logs()\n","                        start_time = time.time()\n","\n","                        generator.eval()\n","\n","                        images, _ = sample_fake_image_batch(self.image_batch_size)\n","                        logger.image_summary(\"Images\", images_to_numpy(images), batch_num)\n","\n","                        videos, _ = sample_fake_video_batch(self.video_batch_size)\n","                        logger.video_summary(\"Videos\", videos_to_numpy(videos), batch_num)\n","\n","                        # Save generator weights every 5 epochs\n","                        if (epoch + 1) % 5 == 0:\n","                            torch.save(generator, os.path.join(self.log_folder, 'generator_%05d.pytorch' % (epoch + 1)))\n","\n","                    pbar.update(1)\n","\n","            # Calculate average losses for the epoch\n","            avg_l_gen = epoch_logs['l_gen'] / batch_num\n","            avg_l_image_dis = epoch_logs['l_image_dis'] / batch_num\n","            avg_l_video_dis = epoch_logs['l_video_dis'] / batch_num\n","\n","            print(f\"Epoch {epoch + 1}/{num_epochs} - Average losses: \"\n","                  f\"Generator: {avg_l_gen:.4f}, \"\n","                  f\"Image Discriminator: {avg_l_image_dis:.4f}, \"\n","                  f\"Video Discriminator: {avg_l_video_dis:.4f}\")\n","\n","            # Log average epoch losses\n","            logger.scalar_summary('avg_l_gen', avg_l_gen, epoch + 1)\n","            logger.scalar_summary('avg_l_image_dis', avg_l_image_dis, epoch + 1)\n","            logger.scalar_summary('avg_l_video_dis', avg_l_video_dis, epoch + 1)\n","\n","            if batch_num >= self.train_batches:\n","                torch.save(generator, os.path.join(self.log_folder, 'generator_%05d.pytorch' % batch_num))\n","                break\n"]},{"cell_type":"markdown","metadata":{"id":"YHBnGHEbYa03"},"source":["#Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPDW2Z2GYaPi"},"outputs":[],"source":["import os\n","import PIL\n","\n","import functools\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","\n","def build_discriminator(type, **kwargs):\n","#    discriminator_type = getattr(models, type)\n","\n","#    if 'Categorical' not in type and 'dim_categorical' in kwargs:\n","#        kwargs.pop('dim_categorical')PatchImageDiscriminator\n","  if type==\"PatchImageDiscriminator\":\n","    return PatchImageDiscriminator(**kwargs)#discriminator_type(**kwargs)\n","  if type==\"CategoricalVideoDiscriminator\":\n","    return CategoricalVideoDiscriminator(**kwargs)#discriminator_type(**kwargs)\n","\n","\n","def video_transform(video, image_transform):\n","    vid = []\n","    for im in video:\n","        vid.append(image_transform(im))\n","\n","    vid = torch.stack(vid).permute(1, 0, 2, 3)\n","\n","    return vid\n","\n","\n","img_size=64\n","video_length = 16\n","image_batch = 10\n","video_batch = 3\n","\n","dim_z_content = 30\n","dim_z_motion = 10\n","dim_z_category = 4\n","print_every=1000\n","batches=100000\n","log_folder=\"./\"\n","use_infogan=0\n","use_categories=3\n","use_noise=0\n","noise_sigma = 0\n","\n","image_discriminator=\"PatchImageDiscriminator\"\n","video_discriminator=\"CategoricalVideoDiscriminator\"\n","\n","n_channels = 3\n","dataset=\"mocogan/data/actions\"\n","\n","\n","def select_channels(x):\n","  return x[:n_channels, ::]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1742231117094,"user":{"displayName":"Alexei Klimovich","userId":"06703062093340586073"},"user_tz":-180},"id":"Yb4ud-WN430S","outputId":"6d071e1c-64f2-433f-8f2a-1f21c09ce62d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of frames 5290\n"]}],"source":["image_transforms = transforms.Compose([\n","    PIL.Image.fromarray,\n","    transforms.Resize(img_size),\n","    transforms.ToTensor(),\n","    select_channels,\n","    transforms.Normalize((0.5, 0.5, .5), (0.5, 0.5, 0.5)),\n","])\n","\n","video_transforms = functools.partial(video_transform, image_transform=image_transforms)\n","\n","dataset = VideoFolderDataset(dataset, cache=os.path.join(dataset, 'local.db'))\n","image_dataset = ImageDataset(dataset, image_transforms)\n","image_loader = DataLoader(image_dataset, batch_size=image_batch, drop_last=True, num_workers=2, shuffle=True)\n","\n","video_dataset = VideoDataset(dataset, 16, 2, video_transforms)\n","video_loader = DataLoader(video_dataset, batch_size=video_batch, drop_last=True, num_workers=2, shuffle=True)\n","\n","generator = VideoGenerator(n_channels, dim_z_content, dim_z_category, dim_z_motion, video_length)\n","\n","image_discriminator = build_discriminator(\n","    image_discriminator, n_channels=n_channels,\n","    use_noise=use_noise, noise_sigma=noise_sigma\n",")\n","\n","video_discriminator = build_discriminator(\n","    video_discriminator, dim_categorical=dim_z_category,\n","    n_channels=n_channels, use_noise=use_noise,\n","    noise_sigma=noise_sigma\n",")\n","\n","if torch.cuda.is_available():\n","    generator.cuda()\n","    image_discriminator.cuda()\n","    video_discriminator.cuda()\n","\n","trainer = Trainer(\n","    image_loader,\n","    video_loader,\n","    print_every,\n","    batches,\n","    log_folder,\n","    use_cuda=torch.cuda.is_available(),\n","    use_infogan=use_infogan,\n","    use_categories=use_categories\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"aZisx0YJ7tli","outputId":"f6bcb346-8a29-4946-8efe-7f680335baf6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/10:   1%|          | 569/100000 [1:00:00<175:06:35,  6.34s/batch]"]}],"source":["num_epochs = 10\n","trainer.train(generator, image_discriminator, video_discriminator, num_epochs)\n"]},{"cell_type":"markdown","metadata":{"id":"KwVH1myP7LAy"},"source":["## Generate videos"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4360,"status":"ok","timestamp":1742230787986,"user":{"displayName":"Alexei Klimovich","userId":"06703062093340586073"},"user_tz":-180},"id":"qo2kb_dYC_j9","outputId":"3f7d8b68-a18e-456d-9868-146b37b8d8ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"]}],"source":["!apt-get install ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":119,"status":"error","timestamp":1742231120927,"user":{"displayName":"Alexei Klimovich","userId":"06703062093340586073"},"user_tz":-180},"id":"pBSWTtp_7Sc6","outputId":"09bd7c2a-6f16-44a9-9d27-ab9dd0bf2dfd"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'generator_20000.pytorch'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-a4df12f5dd56>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'generator_20000.pytorch'"]}],"source":["import os\n","import torch\n","\n","import subprocess as sp\n","\n","model = \"generator_20000.pytorch\"\n","num_videos = 10\n","output_folder = \"./result\"\n","number_of_frames = 16\n","ffmpeg = \"ffmpeg\"\n","output_format = \"gif\"\n","\n","\n","def save_video(ffmpeg, video, filename):\n","    command = [ffmpeg,\n","               '-y',\n","               '-f', 'rawvideo',\n","               '-vcodec', 'rawvideo',\n","               '-s', '64x64',\n","               '-pix_fmt', 'rgb24',\n","               '-r', '8',\n","               '-i', '-',\n","               '-c:v', 'gif',\n","               filename]\n","\n","    pipe = sp.Popen(command, stdin=sp.PIPE, stderr=sp.PIPE, bufsize=0)\n","    pipe.stdin.write(video.tobytes())\n","\n","\n","generator = torch.load(model, map_location={'cuda:0': 'cpu'}, weights_only=False)\n","generator.eval()\n","\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","for i in range(num_videos):\n","    v, _ = generator.sample_videos(1, number_of_frames)\n","    video = videos_to_numpy(v).squeeze().transpose((1, 2, 3, 0))\n","    save_video(ffmpeg, video, os.path.join(output_folder, \"{}.{}\".format(i, output_format)))\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1H59P2LNZPM5pRgjiMPgrjGCDTasv60p5","timestamp":1742042471823}]},"kernelspec":{"display_name":"Python 2","name":"python2"}},"nbformat":4,"nbformat_minor":0}